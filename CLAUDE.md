# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Core Commands

**Installation and Usage:**
```bash
# Installation (one-time setup)
git clone https://github.com/chreez/scrape.git
cd scrape
npm install
npm link                                    # Optional: make globally available

# Usage (after installation)
scrape <url>                                # Extract context from any URL
scrape <url> --output ./data                # Custom output directory
scrape <url> --verbose                      # Debug mode
scrape <url> --no-headless                  # Show browser

# Or direct execution (no npm link needed)
node bin/scrape.js <url>
```

**Development Commands:**
```bash
node src/index.js <url> <data-type>         # Direct usage
npm start                                   # Same as above
npm run lint                                # Code linting (ESLint)
npm test                                    # Run tests (placeholder)
```

**Output Structure:** All extractions create context files in `scrape-output/`:
- `summary.md` - Executive summary
- `content.txt` - Cleaned text content  
- `metadata.json` - Structured metadata
- `context.md` - LLM-optimized context
- `entities.json` - Extracted entities (when detected)

## Architecture Overview

**Two-Track System:**
1. **Smart Mode** (`SmartScraper`): Default NPX interface with AI-powered adaptability
2. **Legacy Mode** (`Scrape`): Platform-specific adapters for targeted extraction

**Smart Extraction Flow:**
```
URL â†’ Page Analysis â†’ Learned Patterns (if available) â†’ Default Extraction â†’ Context Generation
```

**Core Components:**
- **`SmartScraper`**: Main extraction engine with learning capabilities
- **`LearningStorage`**: 6-month persistent cache of successful extraction patterns in `~/.scrape/learning.json`
- **`ContextGenerator`**: Creates LLM-optimized output files
- **`ExtractorEngine`**: Handles different content types (articles, products, profiles, etc.)
- **`StealthManager`**: Anti-detection and browser management

**Platform Detection:** Auto-detects content type from URL patterns and DOM structure:
- Social media (Instagram, Twitter, TikTok)
- Video platforms (YouTube)
- E-commerce (Amazon) 
- Code repositories (GitHub)
- News/articles (generic detection)
- Documentation sites

## Learning System

**Adaptive Learning:** Successfully extracted patterns are cached for 6 months with confidence scoring:
- Stores working selectors, extractor sequences, and optimal timing
- Cache auto-expires and self-repairs on failures
- Confidence decreases over time (100% â†’ 40% over 3 months)
- Failed extractions trigger cache invalidation

**Learning File:** `~/.scrape/learning.json` contains domain-specific patterns with:
- Platform detection results
- Successful extractor types
- Optimal timing parameters
- Working DOM selectors

## File Organization
```
src/
â”œâ”€â”€ smart.js              # Smart extraction with learning
â”œâ”€â”€ index.js              # Legacy platform-specific extraction
â”œâ”€â”€ adapters/             # Platform-specific logic (Instagram, etc.)
â”œâ”€â”€ context/generator.js  # Context file creation
â”œâ”€â”€ extractors/engine.js  # Content type extraction
â”œâ”€â”€ learning/storage.js   # Pattern learning and caching
â”œâ”€â”€ navigation/manager.js # Page navigation strategies
â”œâ”€â”€ stealth/manager.js    # Anti-detection features
â””â”€â”€ orchestrator/         # Task management

bin/scrape.js             # NPX CLI interface
config/platforms.json     # Platform configurations
```

## Development Patterns

**Content Type Strategy:** Different extraction logic based on detected content:
- `article/blog-post/encyclopedia-article` â†’ Article extractor
- `product` â†’ Product extractor  
- `social-profile` â†’ Profile extractor
- `video/social-video` â†’ Video extractor
- `generic` â†’ Fallback text extraction

**Error Handling:** Graceful degradation with learning cache invalidation on failures

**Browser Management:** Playwright-based with stealth features and human-like behavior patterns

## Testing Strategy

**Manual Testing:**
```bash
# Test different content types (after installation)
scrape https://en.wikipedia.org/wiki/AI --verbose
scrape https://news.ycombinator.com/ --verbose
scrape https://github.com/microsoft/playwright --verbose

# Or with direct execution
node bin/scrape.js https://en.wikipedia.org/wiki/AI --verbose
```

**Cleanup:** Always remove test output files (`scrape-output/`) before commits

## Git Workflow

**Commit Format:**
```
type(scope): description

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)
Co-Authored-By: Claude <noreply@anthropic.com>
```

**Prohibited in Commits:**
- Test output files (`*.json` in root, `scrape-output/`)
- Temporary directories
- Debug console.log statements
- Package lock files

**Quality Gates:**
- Keep commits under 300 lines
- Clean workspace before commits
- Push immediately after commit